{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the terminal, create ('python3 -m venv .venv') and activate ('source .venv/bin/activate') the virtual environment, then 'pip3 install -r requirements.txt'\n",
    "- Make sure the correct Python venv is selected in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 363 tensors from /Users/u1155516/Dropbox/Technical/llms/models/llama/llama-mac-metal/llama.cpp/models/llama-2-13b-chat.Q8_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q8_0:  282 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 12.88 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  6594.94 MiB, (16159.58 / 27648.00)\n",
      "llm_load_tensors: offloading 20 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 20/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 13189.86 MiB\n",
      "llm_load_tensors:      Metal buffer size =  6594.93 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3900\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/u1155516/Dropbox/Technical/llms/dev/hand-rolled/.mac_venv/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 28991.03 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1523.44 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  1523.44 MiB, (17683.02 / 27648.00)\n",
      "llama_kv_cache_init:      Metal KV buffer size =  1523.44 MiB\n",
      "llama_new_context_with_model: KV self size  = 3046.88 MiB, K (f16): 1523.44 MiB, V (f16): 1523.44 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    18.65 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   352.31 MiB, (18035.33 / 27648.00)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   352.31 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   354.69 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '40', 'llama.context_length': '4096', 'llama.attention.head_count': '40', 'llama.rope.dimension_count': '128', 'general.file_type': '7', 'llama.feed_forward_length': '13824', 'llama.embedding_length': '5120', 'llama.block_count': '40', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "import os.path, os\n",
    "from llama_index.core import (\n",
    "    Settings,\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    set_global_tokenizer,\n",
    ")\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.llms.llama_cpp.llama_utils import (\n",
    "    messages_to_prompt,\n",
    "    completion_to_prompt,\n",
    ")\n",
    "from transformers import AutoTokenizer\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    # model_url=model_url,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    # model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGML/resolve/main/llama-2-13b-chat.ggmlv3.q4_0.bin\",\n",
    "    model_path=\"/Users/u1155516/Dropbox/Technical/llms/models/llama/llama-mac-metal/llama.cpp/models/llama-2-13b-chat.Q8_0.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 20},\n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12256.89 ms\n",
      "llama_print_timings:      sample time =      16.20 ms /   140 runs   (    0.12 ms per token,  8641.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12256.66 ms /    79 tokens (  155.15 ms per token,     6.45 tokens per second)\n",
      "llama_print_timings:        eval time =   31308.99 ms /   139 runs   (  225.24 ms per token,     4.44 tokens per second)\n",
      "llama_print_timings:       total time =   43799.71 ms /   218 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Of course! Here's a short poem about cats and dogs:\n",
      "\n",
      "Cats and dogs, so different yet so dear,\n",
      "Both bring joy and love, without a fear.\n",
      "\n",
      "Cats purr and curl up at our feet,\n",
      "Dogs wag their tails and give sweet treats.\n",
      "\n",
      "Cats hunt mice with stealthy grace,\n",
      "Dogs chase after balls with eager face.\n",
      "\n",
      "Both are loyal friends through and through,\n",
      "Bringing happiness to me and you.\n",
      "\n",
      "So here's to our feline and canine friends,\n",
      "Both equally loved until the very end.\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(\"Hello! Can you tell me a poem about cats and dogs?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sure! Here's a poem about aliens:\n",
      "\n",
      "In the vastness of space and time,\n",
      "Alien worlds and beings entwine,\n",
      "Their forms and features so divine,\n",
      "A mystery that's all mine.\n",
      "\n",
      "With eyes that glow like stars so bright,\n",
      "And skin that shimmers with delight,\n",
      "They roam the cosmos with grace and might,\n",
      "Their presence fills the space with light.\n",
      "\n",
      "Their ships, like silver wings so sleek,\n",
      "Through galaxies they do seek,\n",
      "Exploring realms unknown to man,\n",
      "Their journey is but just begun.\n",
      "\n",
      "With technologies so advanced and grand,\n",
      "They harness powers of the land,\n",
      "Their wisdom and knowledge so profound,\n",
      "Their secrets yet to be found.\n",
      "\n",
      "In the vastness of space and time,\n",
      "Alien worlds and beings entwine,\n",
      "Their presence fills the cosmos with wonder,\n",
      "A mystery that's all mine."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   12256.89 ms\n",
      "llama_print_timings:      sample time =      26.18 ms /   223 runs   (    0.12 ms per token,  8516.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1166.07 ms /     7 tokens (  166.58 ms per token,     6.00 tokens per second)\n",
      "llama_print_timings:        eval time =   27709.43 ms /   222 runs   (  124.82 ms per token,     8.01 tokens per second)\n",
      "llama_print_timings:       total time =   29356.95 ms /   229 tokens\n"
     ]
    }
   ],
   "source": [
    "response_iter = llm.stream_complete(\"Can you write me a poem about aliens?\")\n",
    "for response in response_iter:\n",
    "    print(response.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ea812fea4a4c3288007d70f4dc31af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3cf4122c7154b95a112fd2feccfb197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c39ff312e34573a019076952390878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e2050baf9a46ddb8315f1f539f6165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the RAG vector index\n",
    "\n",
    "set_global_tokenizer(\n",
    "    AutoTokenizer.from_pretrained(\"NousResearch/Nous-Hermes-Llama2-13b\").encode\n",
    ")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# create vector store index\n",
    "# check if storage already exists\n",
    "if not os.path.exists(\"./storage_au\"):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data_au\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(\"./storage_au\")\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage_au\")\n",
    "    index = load_index_from_storage(storage_context)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   12256.89 ms\n",
      "llama_print_timings:      sample time =      26.48 ms /   256 runs   (    0.10 ms per token,  9669.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   46044.10 ms /  1976 tokens (   23.30 ms per token,    42.92 tokens per second)\n",
      "llama_print_timings:        eval time =   48128.24 ms /   255 runs   (  188.74 ms per token,     5.30 tokens per second)\n",
      "llama_print_timings:       total time =   94712.72 ms /  2231 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Based on the provided context information, I will summarize the Hansard documents for 1901 by themes and highlight the most prominent speakers. Please note that the summaries are based on the provided documents only and do not include any external knowledge or context.\n",
      "\n",
      "1. Themes:\n",
      "\t* Opening of Parliament and proclamations (1901-05-09)\n",
      "\t* Debate on the first speech of the Governor-General (1901-05-09)\n",
      "\t* Discussion on the estimates (1901-05-09)\n",
      "\t* Addresses by the Governor-General and the Prime Minister (1901-05-09)\n",
      "\t* Debate on the Writs of Election (1901-05-09)\n",
      "2. Speakers:\n",
      "\t* Sir GEORGE TURNER (BalaclavaTreasurer)\n",
      "\t* Mr McMILLAN (William)\n",
      "\t* Sir John LANGDON BONYTHON (Kt.)\n",
      "\t* Right Hon. EDMUND BARTON (P.O., K\n"
     ]
    }
   ],
   "source": [
    "# Request a basic summary of the documents in the corpus\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "response = query_engine.query(\"Summarize all of the Hansard documents for 1901 please. It is a big document set so you might want to summarize by themes, noting which sessions emphasised which themes and which speakers spoke the most\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
